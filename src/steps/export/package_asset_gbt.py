
import json
import argparse
import hashlib
import time
import os

def generate_gbt_id(asset_type, year=2026, sequence=1):
    """
    Generate GB/T 36369 compliant Cinema Digital Object Identifier.
    Format: prefix/CN.FILM.ASSET.YYYY.SEQUENCE
    """
    prefix = "10.5000.1" # Hypothetical DOI prefix for China Film Archive
    return f"{prefix}/CN.FILM.ASSET.{year}.{sequence:04d}"

def compute_checksum(file_path):
    hash_md5 = hashlib.md5()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()

def package_asset(input_ply, output_json, local_id, tags=None):
    if tags is None:
        tags = []
        
    gbt_id = generate_gbt_id("PROP", sequence=int(time.time()) % 10000) # Simple mock sequence
    
    file_stat = os.stat(input_ply)
    checksum = compute_checksum(input_ply)
    
    display_format = f"https://doi.org/{gbt_id}"
    
    file_stat = os.stat(input_ply)
    checksum = compute_checksum(input_ply)
    
    
    # --- PHASE 8 UPGRADE: Load Intelligent Decomposition Data ---
    # Look for the .json file generated by harvest_hero_assets.py
    # It should be located in the same directory (or similar path logic)
    # The 'input_ply' is likely in 'output/props_3d/prop_id.ply'
    # The harvest json is in 'output/props/prop_id.json'
    
    # Heuristic: find the 'props' dir instead of 'props_3d'
    ply_dir = os.path.dirname(input_ply)
    props_dir = ply_dir.replace("props_3d", "props")
    harvest_json_path = os.path.join(props_dir, f"{local_id}.json")
    
    structure_info = {}
    if os.path.exists(harvest_json_path):
        try:
            with open(harvest_json_path, 'r') as f:
                h_data = json.load(f)
                structure_info = h_data.get("structure", {})
                print(f"üîó Linked with Intelligent Decomposition Data: {harvest_json_path}")
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to load harvest metadata: {e}")
            
    # GB/T 36369 Appendix B Table B.1 - System Metadata
    metadata = {
        "system_metadata": {
            "doi_name": gbt_id,
            "display_form": display_format,
            "referent_type": "DigitalAsset",   # Type
            "referent_sub_type": "3DModel",    # Sub-type
            "referent_name": local_id,         # Title/Name
            "referent_identifier": [
                {"type": "LocalID", "value": local_id},
                {"type": "MD5", "value": checksum}
            ]
        },
        "technical_metadata": {
            "format": "PLY", 
            "standard": "3DGS-1.0",
            "file_size": file_stat.st_size,
            "created_at": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
        },
        "content_metadata": {
            "tags": tags,
            "description": "Generated via National Platform Intelligent Pipeline",
            "provenance": {
                "source_type": "FilmFootage",
                "extraction_method": "AI-Driven Decomposition"
            },
            "structure_data": structure_info # <--- The Dynamic Data
        }
    }
    
    with open(output_json, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=4, ensure_ascii=False)
        
    print(f"‚úÖ Asset Packaged: {gbt_id}")
    print(f"   Standard: GB/T 36369 (ISO 26324 IDT)")
    print(f"   Authority: National Film Digital Asset Platform (NFDAP)")
    print(f"   Metadata saved to {output_json}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", required=True, help="Path to .ply file")
    parser.add_argument("--id", required=True, help="Internal Local ID")
    parser.add_argument("--tags", nargs="+", default=[], help="List of tags")
    args = parser.parse_args()
    
    output_json = os.path.splitext(args.input)[0] + ".json"
    package_asset(args.input, output_json, args.id, args.tags)
